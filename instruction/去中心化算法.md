下面先帮你把“去中心化”这一块里的算法捋清楚：叫什么、在干什么、怎么干；然后给一个可以直接当作“算法文档”的重写版，你后面可以直接嵌到论文或报告里改改符号就用。

---

## 一、报告里“去中心化”部分的算法梳理

### 1. 总体算法族名称

从报告原文来看，“去中心化强化学习方法”这一章围绕一类统一思路展开，可以概括为：

> **分布式 DTDE 一致性策略优化算法族**
> （DTDE = Decentralized Training & Decentralized Execution）

在这个总框架下，细分为两类算法：

1. **算法 1：离散 MAMDP 的分布式 DTDE 一致性策略优化算法**

   * 面向状态空间、动作空间均离散的多智能体 MDP（MAMDP）。

2. **算法 2：连续 MAMDP 的分布式 DTDE 一致性策略优化算法**

   * 面向状态或动作为连续空间的多智能体任务，采用 Actor–Critic + 神经网络逼近形式。

两者共享同一套核心机制，只是在“值函数/策略函数表示”和“更新方式”上不同。

---

### 2. 核心思想（统一视角）

报告中的关键技术可以压缩为三条核心思路：

1. **局部代理值函数（local surrogate Q）**

   * 把全局动作值函数 (Q(s,a)) 分解为每个智能体的局部代理动作值函数 (Q_i(s,a_i))。
   * 各智能体只用本地信息就能做策略评估与改进，不再依赖“全局 Q 函数 + 中心节点”。

2. **基于重要性采样 + 一致性估计的历史样本重加权**

   * 所有智能体共享一段“历史样本轨迹池”（replay buffer），里面是过去用旧策略采集的轨迹。
   * 由于现在策略已经改变，需要通过**重要性采样权重** (\omega(\tau)) 对旧轨迹进行分布修正。
   * 权重本身依赖所有智能体的局部策略，因此用**分布式一致性算法**（在通信图上做迭代加权平均）来估计全局 log-weight，而不需要中心节点。

3. **去中心化 Actor–Critic / Q-learning 更新**

   * **策略评估（Critic / Q）阶段**：各智能体在本地用**加权后的历史样本**做 TD 误差最小化，更新 (Q_i) 或神经网络 Critic。
   * **策略改进（Actor）阶段**：各智能体基于本地 (Q_i) 和熵正则项，做梯度上升更新局部策略参数，且可证明策略性能单调非减并收敛到最优值的邻域。

整体效果：**不需要中心服务器 + 复用历史样本 + 有收敛性和近优性理论保证**。

---

## 二、新整理的“去中心化算法文档（建议版本）”

下面是一个可以直接当章节/附录放进你论文的算法说明稿，我用比较“教材化”的结构写，你可以按需要剪裁。

---

### 0. 算法总览

**算法族名称：**

> 面向信息不完全多智能体系统的分布式 DTDE 一致性策略优化算法

**适用问题：**

* 模型：多智能体马尔可夫决策过程（MAMDP）；
* 特点：

  * 多智能体、通信拓扑时变；
  * 信息不完全（各智能体只观测局部信息与邻居）；
  * 需要**训练过程去中心化**，执行时也**无中心控制器**；
  * 希望提升数据效率：能充分复用历史轨迹。

---

### 1. 问题建模

1. **单智能体 MDP**：

   * 用五元组 (M=\langle S,A,P,R,\gamma\rangle) 表示：

     * (S)：状态空间；
     * (A)：动作空间；
     * (P(s'|s,a))：转移概率（或密度）；
     * (R(s,a))：即时奖励（均值和方差有界）；
     * (\gamma\in(0,1))：折扣因子。

2. **多智能体 MAMDP**：

   * 系统由 (N) 个智能体组成，通信图为时变的无向图 (\mathcal{G}_t=(\mathcal{V},\mathcal{E}_t))。
   * 全局动作：(a = (a_1,\dots,a_N))，局部动作空间 (A_i)，局部决策空间 (D_i = S \times A_i)。
   * 假设全局状态 (s) 与全局动作 (a) 在系统层面可观（用于理论分析和仿真设定）。

3. **策略与值函数**：

   * 局部策略函数：(\pi_i(a_i|s))，全局策略：(\pi(a|s)=\prod_{i=1}^N \pi_i(a_i|s))。
   * 局部动作值函数：(Q_i^\pi(s,a))，全局动作值函数：(Q^\pi(s,a))。
   * 状态值函数：(V^\pi(s)) 与上述 Q 函数满足标准贝尔曼方程与关系式。

4. **优化目标**：

   * 寻找全局最优策略 (\pi^*)，最大化累积折扣回报
     (\max_{\pi} J(\pi) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t | \pi])。

---

### 2. 算法 1：离散 MAMDP 的分布式 DTDE 一致性策略优化

#### 2.1 算法名称

> **算法 1：离散 MAMDP 分布式 DTDE 一致性策略优化（表格型）**

#### 2.2 算法思路（直观版）

* **表格型表示**：状态与动作均为有限离散集合，动作值函数 (Q_i) 与策略参数 (\theta_i) 都用表格存储。
* **去中心化交互**：

  * 每轮中，各智能体先与邻居交换必要信息，再独立采样局部动作形成全局动作。
* **策略评估（Q 更新）**：

  * 基于采样轨迹计算 TD 误差，用步长 (\alpha_t) 迭代更新表格 Q。
* **策略改进（策略参数更新）**：

  * 策略采取 softmax（或 Boltzmann）形式，用梯度上升在本地提升目标函数值。
* **一致性保证**：

  * 通过合适的步长条件与压缩映射性质，证明 Q 的收敛与策略性能的单调改善。

#### 2.3 算法流程（步骤版）

> **输入：** 状态空间 (S)，动作空间 (A_i)，通信拓扑规则 (\mathcal{G}_t)
> **输出：** 每个智能体的局部最优策略 (\pi_i^*)

1. **初始化**
   1.1 为每个智能体 (i) 初始化表格 (Q_i(s,a)) 与策略参数 (\theta_i)。
   1.2 设置步长序列 ({\alpha_t},{\beta_t}) 满足 Robbins–Monro 条件：
   (\sum_t \alpha_t = \infty,\ \sum_t \alpha_t^2 < \infty) 等。

2. **循环（每一轮交互 / 时间步）**：

   **(A) 交互协同阶段**
   2.1 每个智能体从邻居处接收必要的局部统计量（可选，主要用于分布式估计）。
   2.2 所有智能体基于当前共享状态 (s_t)，按照各自策略 (\pi_i(\cdot|s_t;\theta_i)) 采样局部动作 (a_{i,t})。
   2.3 联合形成全局动作 (a_t=(a_{1,t},\dots,a_{N,t}))，作用于环境获得下一状态 (s_{t+1}) 和即时奖励 (r_t)。

   **(B) 策略评估阶段（更新 (Q_i)）**
   2.4 对每个状态–动作对 ((s_t,a_t))，智能体 (i) 计算 TD 目标：
   [
   y_{i,t} = r_t + \gamma \sum_{a'*i} \pi_i(a'*i|s*{t+1};\theta_i), Q_i(s*{t+1},a'*i)
   ]
   2.5 更新局部 Q 表：
   [
   Q_i(s_t,a*{i,t}) \leftarrow Q_i(s_t,a_{i,t}) + \alpha_t\big( y_{i,t}- Q_i(s_t,a_{i,t}) \big)
   ]

   **(C) 策略改进阶段（更新 (\theta_i)）**
   2.6 设策略目标函数类似
   [
   J_i(\theta_i)= \mathbb{E}*{\pi}\big[ Q_i(s,a_i) \big]
   ]
   2.7 每个智能体用策略梯度更新参数：
   [
   \theta_i \leftarrow \theta_i + \beta_t \nabla*{\theta_i} J_i(\theta_i)
   ]

3. **重复步骤 2，直到收敛或达到最大迭代轮数。**

> 注：原文中的“算法 1”正是上述流程的形式化版本，包含更细致的轨迹采集与参数更新细节。

---

### 3. 算法 2：连续 MAMDP 的分布式 DTDE 一致性策略优化

#### 3.1 算法名称

> **算法 2：连续 MAMDP 分布式 DTDE 一致性 Actor–Critic 算法**

#### 3.2 算法思路（直观版）

这里就是“真正的去中心化深度强化学习算法”，对应报告中的 Algorithm 2：

1. **Actor–Critic 架构 + 连续动作空间**

   * 每个智能体 (i) 有：

     * 策略网络（Actor）(\pi_{\theta_i}(a_i|s))；
     * 局部代理动作值函数网络（Critic）(Q_{\phi_i}(s,a_i))。

2. **历史样本重用（经验回放池）**

   * 所有智能体共享一个或逻辑上统一的“轨迹池”，存放历史轨迹 (\tau = (s,a,r,s'))。
   * 利用重要性采样权重 (\omega(\tau)) 对轨迹做分布修正，使得用旧数据也能评估当前策略。

3. **权重的分布式一致性估计**

   * 轨迹权重涉及所有智能体的局部策略 (\pi_i)，因此每个智能体先计算自己的局部 log-weight 再通过一致性算法做平均，得到全局 log-weight 的近似。

4. **策略评估阶段（Critic 更新）**

   * 基于加权样本最小化 TD 残差（可用 Double Q + 目标网络）来稳定训练：
   * 采用截断 Double Q-learning 和软更新目标网络参数。

5. **策略改进阶段（Actor 更新）**

   * 使用熵正则的 Actor 目标
     [
     J_i(\theta_i)=\mathbb{E}[Q_{\phi_i}(s,a_i) + \alpha \mathcal{H}(\pi_{\theta_i}(\cdot|s))]
     ]
   * 采用重参数化技巧（如高斯策略 (a_i = \mu_{\theta_i}(s) + \sigma_{\theta_i}(s)\odot \epsilon)）计算梯度，实现稳定的端到端反向传播。

#### 3.3 算法流程（带阶段划分）

> **输入：** 连续状态空间 (S)，动作空间 (A_i)，Actor/Critic 网络结构，通信图 (\mathcal{G}_t)
> **输出：** 各智能体的策略参数 (\theta_i^*)

---

#### 阶段 0：初始化

0.1 为每个智能体初始化 Actor 网络 (\pi_{\theta_i}) 与 Critic 网络 (Q_{\phi_i})，以及对应的目标网络 (Q_{\bar{\phi}_i})。
0.2 初始化历史样本轨迹池 (\mathcal{D})，设置最大容量 (M)。

---

#### 阶段 1：交互协同（数据采集）

重复以下过程：

1.1 给定当前状态 (s_t)，每个智能体独立采样局部动作：
[
a_{i,t} \sim \pi_{\theta_i}( \cdot | s_t)
]

1.2 组合得到全局动作 (a_t=(a_{1,t},\dots,a_{N,t}))，环境执行后得到下一个状态 (s_{t+1}) 和奖励 (r_t)。

1.3 将样本 ((s_t,a_t,r_t,s_{t+1})) 存入轨迹池 (\mathcal{D})，如果超出容量则丢弃最旧样本。

---

#### 阶段 2：历史轨迹权重的一致性估计

2.1 从轨迹池中采样一小批轨迹子集 (\mathcal{B} \subset \mathcal{D})。

2.2 对每条轨迹 (\tau\in\mathcal{B})，每个智能体在本地计算**局部对数权重**：
[
\ell_{i}(\tau) = \log \frac{\pi_{\theta_i}(a_i|s)}{\pi_{\theta_i^{\text{old}}}(a_i|s)}
]

2.3 使用一致性算法在通信图 (\mathcal{G}_t) 上迭代：

* 初值：(z_i^{(0)}(\tau) = \ell_i(\tau))
* 迭代：
  [
  z_i^{(k+1)}(\tau) = \sum_{j\in \mathcal{N}*i\cup{i}} w*{ij} z_j^{(k)}(\tau)
  ]
  其中 (w_{ij}) 为满足行随机性与连通性的权重。

2.4 收敛后，(z_i^{(\infty)}(\tau)) 近似全局 log-weight 的均值：
[
\log \omega(\tau) \approx \sum_{i=1}^N \ell_i(\tau) = N \cdot z_i^{(\infty)}(\tau)
]

2.5 各智能体据此在本地计算权重 (\omega(\tau)=\exp(\log \omega(\tau)))，进行轨迹重加权。

---

#### 阶段 3：策略评估（Critic 更新）

3.1 对每个智能体 (i)，在批次 (\mathcal{B}) 上构造加权 TD 目标：
[
y_{i} = r + \gamma \min_{k=1,2} Q_{\bar{\phi}*i^{(k)}}(s', a_i'),\quad a_i'\sim \pi*{\theta_i}( \cdot | s')
]

3.2 构造残差损失：
[
L_i(\phi_i) = \mathbb{E}*{\tau\in\mathcal{B}} \big[ \omega(\tau), (Q*{\phi_i}(s,a_i)-y_i)^2 \big]
]

3.3 对 (\phi_i) 用随机梯度下降（加权）更新：
[
\phi_i \leftarrow \phi_i - \eta_{\phi}\nabla_{\phi_i} L_i(\phi_i)
]

3.4 定期用软更新方式更新目标网络：
[
\bar{\phi}_i \leftarrow \tau \phi_i + (1-\tau)\bar{\phi}_i
]

---

#### 阶段 4：策略改进（Actor 更新）

4.1 构造 Actor 目标（包含熵正则）：
[
J_i(\theta_i) = \mathbb{E}*{s\sim \mathcal{B}, \epsilon\sim\mathcal{N}(0,I)}\big[ Q*{\phi_i}(s,a_i(s,\epsilon)) + \alpha \mathcal{H}(\pi_{\theta_i}(\cdot|s)) \big]
]

4.2 使用重参数化形式采样动作：
[
a_i(s,\epsilon) = \mu_{\theta_i}(s) + \sigma_{\theta_i}(s)\odot \epsilon
]

4.3 使用加权的策略梯度更新 Actor：
[
\theta_i \leftarrow \theta_i + \eta_{\theta} \nabla_{\theta_i} J_i(\theta_i)
]

---

#### 阶段 5：循环与停止条件

5.1 重复执行“交互协同–权重估计–Critic 更新–Actor 更新”四个阶段，直到：

* 策略性能曲线收敛；或
* 达到预设训练轮数。

> 理论上，在合适的步长与熵正则系数下，算法 2 的策略性能单调上升，并收敛到全局最优值的一个小邻域。

---

### 4. 小结：一句话对外描述

> 本文在多智能体 MDP 框架下提出了一类**分布式 DTDE 一致性策略优化算法**，在离散与连续状态–动作空间下分别给出了基于表格 Q-learning 与 Actor–Critic 的两种实现。该算法通过“局部代理值函数 + 重要性采样 + 一致性参数估计”实现历史样本的分布式加权重用，使各智能体在仅依赖局部信息与邻居通信的条件下完成策略评估与策略改进，并在理论上证明了算法的收敛性与性能近优性。
